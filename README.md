
# From Brittle to Robust: Improving LLM Annotations for SE Optimization 


## Abstract
Software analytics often builds 
from labelled data.
Labeling   can be slow, error prone, and expensive.
When human expertise is scarce,
SE researchers sometimes
ask large language models (LLMs) for the   missing labels.

While this has been successful in some domains,
recent results show that LLM-based  labeling has blind spots.
Specifically, their labeling is not effective for  higher dimensional multi-objective problems.


To address this task, we propose a novel LLM prompting strategy called SynthCore. When one opinion fails,  SynthCore's   combines multiple separated opinions generated by LLMs (with no knowledge of each others' answers) into
an ensemble of few-shot learners. 
Simpler than other strategies (e.g. chain-of-thought, multi-agent-debate, etc)   SynthCore   aggregates results from    multiple single prompt sessions (with no crossover between them). 

  SynthCore has been tested on 49 SE multi-objective optimization tasks,
 handling tasks as diverse as software project management, Makefile configuration, and hyperparameter optimization.
SynthCore's
ensemble   found optimizations
better than state-of-the-art alternative approaches  (Gaussian Process Models, Tree of Parzen Estimators,
active learners in both explore and exploit mode). 
Importantly, these optimizations were made using data labelled by LLMs, without any human opinions.

From these experiments, we conclude that ensembles of few shot learners can successfully annotate high dimensional multi-objective tasks. Further, we speculate that other successful few-short prompting results could be quickly and easily enhanced  using SynthCore's ensemble approach.


This repo geenrates the reports needed to address research questions 1,2 fro our recent paper [![PDF Badge](https://img.shields.io/badge/View%20PDF-red?style=for-the-badge&logo=adobeacrobatreader&logoColor=white)](https://github.com/lohithsowmiyan/lazy-llm/blob/main/docs/paper.pdf) on combining LLMs with active learning for SE multi-objective optimization problems.
## Environment Setup


```bash
#clone the repository
git clone https://github.com/lohithsowmiyan/lazy-llm.git
cd lazy-llm

#Install necessary modules
pip install -r requirements.txt --no-warn-script-location

#create a .env file and add all the necessary tokens
touch .env

## inside the .env file add these environemt variables
HF_TOKEN = XXXXX XXXXX (Your token from Huggingface)
GOOGLE_API_KEY = XXXXX XXXXX (Your key for gemini)
OPENAI_API_KEY = XXXXX XXXXX (Your key from Open AI)
```





## Usage/Examples

```bash
#Example 1
python lazy.py --model vanilla --llm llama3-8b --dataset data/misc/auto93.csv
#Example 2
python lazy.py --model vanilla --llm gpt-3.5-turbo --dataset data/misc/wine_quality.csv
```

## Run Entire Experiment

```bash
make RQ123
```


## Configurations

#### Experiment Settings

| Parameter | Values     | Description                |
| :-------- | :------- | :------------------------- |
| `model` | `vanilla` | Simple Greedy Selector (LLM) |
| `model` | `smo` | Sequential Model Optimization (Baseline) |
| `llm` | `llama3-8b`, `gemini-pro`, `phi3-medium`, `gpt-4`  | **Can add any model via llm.py file** |
| `dataset` | `data/misc/auto93.csv` | Enter the full path for any of the datasets in repository|
| `explanation`      | `true` | Log / Display the rational behind the model decisons for every label |

#### LLM Settings

| Parameter | Type     | Description                       |
| :-------- | :------- | :-------------------------------- |
| `temperature`      | `0 - 1` | Controls the creativity of the model |
| `max_tokens`      | `50` | Leave it at default values |
| `top_p`      | `0 - 1` | Focus levels on the core of the prompt |



#### Optional Settings

| Parameter | Type     | Description                       |
| :-------- | :------- | :-------------------------------- |
| `quantization`      | `True` or  `False` | May affect  model performance |
| `q_bits`      | `4` or  `8` | Lower value results in low operating cost |


## Visualization/Examples

```bash
#Example 1
python3 graph.py auto93.csv All
#Example 2
python3 graph.py healthCloseIsses12mths0011-easy Mu
```


## Acknowledgements

 - [EZR](https://github.com/timm/ezr/tree/24Jun14?tab=readme-ov-file)

